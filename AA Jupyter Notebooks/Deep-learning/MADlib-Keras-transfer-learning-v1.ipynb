{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Using Keras and MADlib\n",
    "\n",
    "This is a transfer learning example based on https://keras.io/examples/mnist_transfer_cnn/ \n",
    "\n",
    "## Table of contents\n",
    "<a href=\"#import_libraries\">1. Import libraries</a>\n",
    "\n",
    "<a href=\"#load_and_prepare_data\">2. Load and prepare data</a>\n",
    "\n",
    "<a href=\"#image_preproc\">3. Call image preprocessor</a>\n",
    "\n",
    "<a href=\"#define_and_load_model\">4. Define and load model architecture</a>\n",
    "\n",
    "<a href=\"#train\">5. Train</a>\n",
    "\n",
    "<a href=\"#transfer_learning\">6. Transfer learning</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fmcquillan/anaconda/lib/python2.7/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.\n",
      "  \"You should import from traitlets.config instead.\", ShimWarning)\n",
      "/Users/fmcquillan/anaconda/lib/python2.7/site-packages/IPython/utils/traitlets.py:5: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  warn(\"IPython.utils.traitlets has moved to a top-level traitlets package.\")\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Connected: gpadmin@madlib'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Greenplum Database 5.x on GCP (PM demo machine)\n",
    "#%sql postgresql://gpadmin@35.184.232.200:5432/madlib\n",
    "  \n",
    "# Greenplum Database 5.x on GCP for deep learning (PM demo machine)\n",
    "%sql postgresql://gpadmin@35.239.240.26:5432/madlib\n",
    "        \n",
    "# PostgreSQL local\n",
    "#%sql postgresql://fmcquillan@localhost:5432/madlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>version</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>MADlib version: 1.16-dev, git revision: rel/v1.15.1-129-g954609a, cmake configuration time: Fri Jun 21 18:18:35 UTC 2019, build type: release, build system: Linux-3.10.0-957.12.1.el7.x86_64, C compiler: gcc 4.8.5, C++ compiler: g++ 4.8.5</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(u'MADlib version: 1.16-dev, git revision: rel/v1.15.1-129-g954609a, cmake configuration time: Fri Jun 21 18:18:35 UTC 2019, build type: release, build system: Linux-3.10.0-957.12.1.el7.x86_64, C compiler: gcc 4.8.5, C++ compiler: g++ 4.8.5',)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql select madlib.version();\n",
    "#%sql select version();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"import_libraries\"></a>\n",
    "# 1.  Import libraries\n",
    "From https://keras.io/examples/mnist_transfer_cnn/ import libraries and define some params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "now = datetime.datetime.now\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 5\n",
    "epochs = 5\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of convolutional filters to use\n",
    "filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = 2\n",
    "# convolution kernel size\n",
    "kernel_size = 3\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others needed in this workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load_and_prepare_data\"></a>\n",
    "# 2.  Load and prepare data\n",
    "\n",
    "First load MNIST data from Keras, consisting of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4861, 28, 28)\n",
      "(4861, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create two datasets one with digits below 5 and one with 5 and above\n",
    "x_train_lt5 = x_train[y_train < 5]\n",
    "y_train_lt5 = y_train[y_train < 5]\n",
    "x_test_lt5 = x_test[y_test < 5]\n",
    "y_test_lt5 = y_test[y_test < 5]\n",
    "\n",
    "x_train_gte5 = x_train[y_train >= 5]\n",
    "y_train_gte5 = y_train[y_train >= 5] - 5\n",
    "x_test_gte5 = x_test[y_test >= 5]\n",
    "y_test_gte5 = y_test[y_test >= 5] - 5\n",
    "\n",
    "# reshape to match model architecture\n",
    "print(x_test_gte5.shape)\n",
    "x_train_lt5=x_train_lt5.reshape(len(x_train_lt5), *input_shape)\n",
    "x_test_lt5 = x_test_lt5.reshape(len(x_test_lt5), *input_shape)\n",
    "x_train_gte5=x_train_gte5.reshape(len(x_train_gte5), *input_shape)\n",
    "x_test_gte5 = x_test_gte5.reshape(len(x_test_gte5), *input_shape)\n",
    "print(x_test_gte5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets into tables using image loader scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MADlib tools directory\n",
    "import sys\n",
    "import os\n",
    "madlib_site_dir = '/Users/fmcquillan/Documents/Product/MADlib/Demos/data'\n",
    "sys.path.append(madlib_site_dir)\n",
    "\n",
    "# Import image loader module\n",
    "from madlib_image_loader import ImageLoader, DbCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify database credentials, for connecting to db\n",
    "db_creds = DbCredentials(user='gpadmin',\n",
    "                         host='35.239.240.26',\n",
    "                         port='5432',\n",
    "                         password='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ImageLoader (increase num_workers to run faster)\n",
    "iloader = ImageLoader(num_workers=5, db_creds=db_creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Executing: CREATE TABLE train_lt5 (id SERIAL, x REAL[], y TEXT)\n",
      "CREATE TABLE\n",
      "Created table train_lt5 in madlib db\n",
      "Spawning 5 workers...\n",
      "Initializing PoolWorker-31 [pid 36504]\n",
      "PoolWorker-31: Created temporary directory PoolWorker-31\n",
      "Initializing PoolWorker-32 [pid 36505]\n",
      "PoolWorker-32: Created temporary directory PoolWorker-32\n",
      "Initializing PoolWorker-33 [pid 36506]\n",
      "PoolWorker-33: Created temporary directory PoolWorker-33\n",
      "Initializing PoolWorker-34 [pid 36507]\n",
      "PoolWorker-34: Created temporary directory PoolWorker-34\n",
      "Initializing PoolWorker-35 [pid 36508]\n",
      "PoolWorker-35: Created temporary directory PoolWorker-35\n",
      "PoolWorker-31: Connected to madlib db.\n",
      "PoolWorker-32: Connected to madlib db.\n",
      "PoolWorker-33: Connected to madlib db.\n",
      "PoolWorker-34: Connected to madlib db.\n",
      "PoolWorker-35: Connected to madlib db.\n",
      "PoolWorker-32: Wrote 1000 images to /tmp/madlib_BaxJbXtAOV/train_lt50000.tmp\n",
      "PoolWorker-31: Wrote 1000 images to /tmp/madlib_gijRoaCfJc/train_lt50000.tmp\n",
      "PoolWorker-33: Wrote 1000 images to /tmp/madlib_OpZNic57mi/train_lt50000.tmp\n",
      "PoolWorker-35: Wrote 1000 images to /tmp/madlib_l9HyKE46Lp/train_lt50000.tmp\n",
      "PoolWorker-34: Wrote 1000 images to /tmp/madlib_92vUGUI7Kg/train_lt50000.tmp\n",
      "PoolWorker-31: Loaded 1000 images into train_lt5\n",
      "PoolWorker-32: Loaded 1000 images into train_lt5\n",
      "PoolWorker-34: Loaded 1000 images into train_lt5\n",
      "PoolWorker-33: Loaded 1000 images into train_lt5\n",
      "PoolWorker-31: Wrote 1000 images to /tmp/madlib_gijRoaCfJc/train_lt50001.tmp\n",
      "PoolWorker-35: Loaded 1000 images into train_lt5\n",
      "PoolWorker-32: Wrote 1000 images to /tmp/madlib_BaxJbXtAOV/train_lt50001.tmp\n",
      "PoolWorker-34: Wrote 1000 images to /tmp/madlib_92vUGUI7Kg/train_lt50001.tmp\n",
      "PoolWorker-33: Wrote 1000 images to /tmp/madlib_OpZNic57mi/train_lt50001.tmp\n",
      "PoolWorker-35: Wrote 1000 images to /tmp/madlib_l9HyKE46Lp/train_lt50001.tmp\n",
      "PoolWorker-31: Loaded 1000 images into train_lt5\n",
      "PoolWorker-32: Loaded 1000 images into train_lt5\n",
      "PoolWorker-34: Loaded 1000 images into train_lt5\n",
      "PoolWorker-31: Wrote 1000 images to /tmp/madlib_gijRoaCfJc/train_lt50002.tmp\n",
      "PoolWorker-32: Wrote 1000 images to /tmp/madlib_BaxJbXtAOV/train_lt50002.tmp\n",
      "PoolWorker-33: Loaded 1000 images into train_lt5\n",
      "PoolWorker-34: Wrote 1000 images to /tmp/madlib_92vUGUI7Kg/train_lt50002.tmp\n",
      "PoolWorker-35: Loaded 1000 images into train_lt5\n",
      "PoolWorker-33: Wrote 1000 images to /tmp/madlib_OpZNic57mi/train_lt50002.tmp\n",
      "PoolWorker-31: Loaded 1000 images into train_lt5\n",
      "PoolWorker-35: Wrote 1000 images to /tmp/madlib_l9HyKE46Lp/train_lt50002.tmp\n",
      "PoolWorker-34: Loaded 1000 images into train_lt5\n",
      "PoolWorker-32: Loaded 1000 images into train_lt5\n",
      "PoolWorker-31: Wrote 1000 images to /tmp/madlib_gijRoaCfJc/train_lt50003.tmp\n",
      "PoolWorker-34: Wrote 1000 images to /tmp/madlib_92vUGUI7Kg/train_lt50003.tmp\n",
      "PoolWorker-32: Wrote 1000 images to /tmp/madlib_BaxJbXtAOV/train_lt50003.tmp\n",
      "PoolWorker-33: Loaded 1000 images into train_lt5\n",
      "PoolWorker-31: Loaded 1000 images into train_lt5\n",
      "PoolWorker-33: Wrote 1000 images to /tmp/madlib_OpZNic57mi/train_lt50003.tmp\n",
      "PoolWorker-35: Loaded 1000 images into train_lt5\n",
      "PoolWorker-31: Wrote 1000 images to /tmp/madlib_gijRoaCfJc/train_lt50004.tmp\n",
      "PoolWorker-34: Loaded 1000 images into train_lt5\n",
      "PoolWorker-35: Wrote 1000 images to /tmp/madlib_l9HyKE46Lp/train_lt50003.tmp\n",
      "PoolWorker-32: Loaded 1000 images into train_lt5\n",
      "PoolWorker-34: Wrote 1000 images to /tmp/madlib_92vUGUI7Kg/train_lt50004.tmp\n",
      "PoolWorker-31: Loaded 1000 images into train_lt5\n",
      "PoolWorker-33: Loaded 1000 images into train_lt5\n",
      "PoolWorker-32: Wrote 1000 images to /tmp/madlib_BaxJbXtAOV/train_lt50004.tmp\n",
      "PoolWorker-31: Wrote 1000 images to /tmp/madlib_gijRoaCfJc/train_lt50005.tmp\n",
      "PoolWorker-33: Wrote 1000 images to /tmp/madlib_OpZNic57mi/train_lt50004.tmp\n",
      "PoolWorker-34: Loaded 1000 images into train_lt5\n",
      "PoolWorker-35: Loaded 1000 images into train_lt5\n",
      "PoolWorker-32: Loaded 1000 images into train_lt5\n",
      "PoolWorker-31: Loaded 1000 images into train_lt5\n",
      "PoolWorker-35: Wrote 1000 images to /tmp/madlib_l9HyKE46Lp/train_lt50004.tmp\n",
      "PoolWorker-34: Wrote 1000 images to /tmp/madlib_92vUGUI7Kg/train_lt50005.tmp\n",
      "PoolWorker-31: Wrote 596 images to /tmp/madlib_gijRoaCfJc/train_lt50006.tmp\n",
      "PoolWorker-33: Loaded 1000 images into train_lt5\n",
      "PoolWorker-32: Wrote 1000 images to /tmp/madlib_BaxJbXtAOV/train_lt50005.tmp\n",
      "PoolWorker-33: Wrote 1000 images to /tmp/madlib_OpZNic57mi/train_lt50005.tmp\n",
      "PoolWorker-31: Loaded 596 images into train_lt5\n",
      "PoolWorker-34: Loaded 1000 images into train_lt5\n",
      "PoolWorker-32: Loaded 1000 images into train_lt5\n",
      "PoolWorker-35: Loaded 1000 images into train_lt5\n",
      "PoolWorker-33: Loaded 1000 images into train_lt5\n",
      "PoolWorker-35: Wrote 1000 images to /tmp/madlib_l9HyKE46Lp/train_lt50005.tmp\n",
      "PoolWorker-35: Loaded 1000 images into train_lt5\n",
      "PoolWorker-35: Removed temporary directory PoolWorker-35\n",
      "PoolWorker-34: Removed temporary directory PoolWorker-34\n",
      "PoolWorker-33: Removed temporary directory PoolWorker-33\n",
      "PoolWorker-31: Removed temporary directory PoolWorker-31\n",
      "PoolWorker-32: Removed temporary directory PoolWorker-32\n",
      "Done!  Loaded 30596 images in 13.3909709454s\n",
      "MainProcess: Connected to madlib db.\n",
      "Executing: CREATE TABLE test_lt5 (id SERIAL, x REAL[], y TEXT)\n",
      "CREATE TABLE\n",
      "Created table test_lt5 in madlib db\n",
      "Spawning 5 workers...\n",
      "Initializing PoolWorker-36 [pid 36510]\n",
      "PoolWorker-36: Created temporary directory PoolWorker-36\n",
      "Initializing PoolWorker-37 [pid 36511]\n",
      "PoolWorker-37: Created temporary directory PoolWorker-37\n",
      "Initializing PoolWorker-38 [pid 36512]\n",
      "PoolWorker-38: Created temporary directory PoolWorker-38\n",
      "Initializing PoolWorker-39 [pid 36513]\n",
      "PoolWorker-39: Created temporary directory PoolWorker-39\n",
      "Initializing PoolWorker-40 [pid 36514]\n",
      "PoolWorker-40: Created temporary directory PoolWorker-40\n",
      "PoolWorker-36: Connected to madlib db.\n",
      "PoolWorker-37: Connected to madlib db.\n",
      "PoolWorker-38: Connected to madlib db.\n",
      "PoolWorker-39: Connected to madlib db.\n",
      "PoolWorker-40: Connected to madlib db.\n",
      "PoolWorker-37: Wrote 1000 images to /tmp/madlib_TPjU7lz34X/test_lt50000.tmp\n",
      "PoolWorker-38: Wrote 1000 images to /tmp/madlib_PNUF8ABUjF/test_lt50000.tmp\n",
      "PoolWorker-39: Wrote 1000 images to /tmp/madlib_4X2ZzvvwFm/test_lt50000.tmp\n",
      "PoolWorker-40: Wrote 1000 images to /tmp/madlib_neDGAgy83W/test_lt50000.tmp\n",
      "PoolWorker-36: Wrote 1000 images to /tmp/madlib_L6JjsmxTxZ/test_lt50000.tmp\n",
      "PoolWorker-36: Loaded 1000 images into test_lt5\n",
      "PoolWorker-37: Loaded 1000 images into test_lt5\n",
      "PoolWorker-36: Wrote 139 images to /tmp/madlib_L6JjsmxTxZ/test_lt50001.tmp\n",
      "PoolWorker-38: Loaded 1000 images into test_lt5\n",
      "PoolWorker-40: Loaded 1000 images into test_lt5\n",
      "PoolWorker-36: Loaded 139 images into test_lt5\n",
      "PoolWorker-39: Loaded 1000 images into test_lt5\n",
      "PoolWorker-38: Removed temporary directory PoolWorker-38\n",
      "PoolWorker-37: Removed temporary directory PoolWorker-37\n",
      "PoolWorker-36: Removed temporary directory PoolWorker-36\n",
      "PoolWorker-40: Removed temporary directory PoolWorker-40\n",
      "PoolWorker-39: Removed temporary directory PoolWorker-39\n",
      "Done!  Loaded 5139 images in 3.34779882431s\n",
      "MainProcess: Connected to madlib db.\n",
      "Executing: CREATE TABLE train_gte5 (id SERIAL, x REAL[], y TEXT)\n",
      "CREATE TABLE\n",
      "Created table train_gte5 in madlib db\n",
      "Spawning 5 workers...\n",
      "Initializing PoolWorker-41 [pid 36515]\n",
      "PoolWorker-41: Created temporary directory PoolWorker-41\n",
      "Initializing PoolWorker-42 [pid 36516]\n",
      "PoolWorker-42: Created temporary directory PoolWorker-42\n",
      "Initializing PoolWorker-43 [pid 36517]\n",
      "PoolWorker-43: Created temporary directory PoolWorker-43\n",
      "Initializing PoolWorker-44 [pid 36518]\n",
      "PoolWorker-44: Created temporary directory PoolWorker-44\n",
      "Initializing PoolWorker-45 [pid 36519]\n",
      "PoolWorker-45: Created temporary directory PoolWorker-45\n",
      "PoolWorker-41: Connected to madlib db.\n",
      "PoolWorker-42: Connected to madlib db.\n",
      "PoolWorker-43: Connected to madlib db.\n",
      "PoolWorker-44: Connected to madlib db.\n",
      "PoolWorker-45: Connected to madlib db.\n",
      "PoolWorker-41: Wrote 1000 images to /tmp/madlib_4culpKfGHi/train_gte50000.tmp\n",
      "PoolWorker-42: Wrote 1000 images to /tmp/madlib_V6Nqwp374Q/train_gte50000.tmp\n",
      "PoolWorker-43: Wrote 1000 images to /tmp/madlib_fHsDrEhTBh/train_gte50000.tmp\n",
      "PoolWorker-44: Wrote 1000 images to /tmp/madlib_CU2CNf1Yxz/train_gte50000.tmp\n",
      "PoolWorker-45: Wrote 1000 images to /tmp/madlib_1ZkWBU7pyz/train_gte50000.tmp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoolWorker-43: Loaded 1000 images into train_gte5\n",
      "PoolWorker-44: Loaded 1000 images into train_gte5\n",
      "PoolWorker-41: Loaded 1000 images into train_gte5\n",
      "PoolWorker-43: Wrote 1000 images to /tmp/madlib_fHsDrEhTBh/train_gte50001.tmp\n",
      "PoolWorker-44: Wrote 1000 images to /tmp/madlib_CU2CNf1Yxz/train_gte50001.tmp\n",
      "PoolWorker-41: Wrote 1000 images to /tmp/madlib_4culpKfGHi/train_gte50001.tmp\n",
      "PoolWorker-43: Loaded 1000 images into train_gte5\n",
      "PoolWorker-42: Loaded 1000 images into train_gte5\n",
      "PoolWorker-45: Loaded 1000 images into train_gte5\n",
      "PoolWorker-43: Wrote 1000 images to /tmp/madlib_fHsDrEhTBh/train_gte50002.tmp\n",
      "PoolWorker-41: Loaded 1000 images into train_gte5\n",
      "PoolWorker-42: Wrote 1000 images to /tmp/madlib_V6Nqwp374Q/train_gte50001.tmp\n",
      "PoolWorker-45: Wrote 1000 images to /tmp/madlib_1ZkWBU7pyz/train_gte50001.tmp\n",
      "PoolWorker-41: Wrote 1000 images to /tmp/madlib_4culpKfGHi/train_gte50002.tmp\n",
      "PoolWorker-43: Loaded 1000 images into train_gte5\n",
      "PoolWorker-44: Loaded 1000 images into train_gte5\n",
      "PoolWorker-43: Wrote 1000 images to /tmp/madlib_fHsDrEhTBh/train_gte50003.tmp\n",
      "PoolWorker-44: Wrote 1000 images to /tmp/madlib_CU2CNf1Yxz/train_gte50002.tmp\n",
      "PoolWorker-41: Loaded 1000 images into train_gte5\n",
      "PoolWorker-42: Loaded 1000 images into train_gte5\n",
      "PoolWorker-43: Loaded 1000 images into train_gte5\n",
      "PoolWorker-41: Wrote 1000 images to /tmp/madlib_4culpKfGHi/train_gte50003.tmp\n",
      "PoolWorker-45: Loaded 1000 images into train_gte5\n",
      "PoolWorker-42: Wrote 1000 images to /tmp/madlib_V6Nqwp374Q/train_gte50002.tmp\n",
      "PoolWorker-43: Wrote 1000 images to /tmp/madlib_fHsDrEhTBh/train_gte50004.tmp\n",
      "PoolWorker-44: Loaded 1000 images into train_gte5\n",
      "PoolWorker-45: Wrote 1000 images to /tmp/madlib_1ZkWBU7pyz/train_gte50002.tmp\n",
      "PoolWorker-41: Loaded 1000 images into train_gte5\n",
      "PoolWorker-43: Loaded 1000 images into train_gte5\n",
      "PoolWorker-44: Wrote 1000 images to /tmp/madlib_CU2CNf1Yxz/train_gte50003.tmp\n",
      "PoolWorker-42: Loaded 1000 images into train_gte5\n",
      "PoolWorker-41: Wrote 1000 images to /tmp/madlib_4culpKfGHi/train_gte50004.tmp\n",
      "PoolWorker-43: Wrote 1000 images to /tmp/madlib_fHsDrEhTBh/train_gte50005.tmp\n",
      "PoolWorker-42: Wrote 1000 images to /tmp/madlib_V6Nqwp374Q/train_gte50003.tmp\n",
      "PoolWorker-43: Loaded 1000 images into train_gte5\n",
      "PoolWorker-41: Loaded 1000 images into train_gte5\n",
      "PoolWorker-45: Loaded 1000 images into train_gte5\n",
      "PoolWorker-44: Loaded 1000 images into train_gte5\n",
      "PoolWorker-42: Loaded 1000 images into train_gte5\n",
      "PoolWorker-43: Wrote 1000 images to /tmp/madlib_fHsDrEhTBh/train_gte50006.tmp\n",
      "PoolWorker-41: Wrote 1000 images to /tmp/madlib_4culpKfGHi/train_gte50005.tmp\n",
      "PoolWorker-45: Wrote 1000 images to /tmp/madlib_1ZkWBU7pyz/train_gte50003.tmp\n",
      "PoolWorker-44: Wrote 1000 images to /tmp/madlib_CU2CNf1Yxz/train_gte50004.tmp\n",
      "PoolWorker-42: Wrote 1000 images to /tmp/madlib_V6Nqwp374Q/train_gte50004.tmp\n",
      "PoolWorker-43: Loaded 1000 images into train_gte5\n",
      "PoolWorker-41: Loaded 1000 images into train_gte5\n",
      "PoolWorker-43: Wrote 1000 images to /tmp/madlib_fHsDrEhTBh/train_gte50007.tmp\n",
      "PoolWorker-42: Loaded 1000 images into train_gte5\n",
      "PoolWorker-44: Loaded 1000 images into train_gte5\n",
      "PoolWorker-45: Loaded 1000 images into train_gte5\n",
      "PoolWorker-42: Wrote 404 images to /tmp/madlib_V6Nqwp374Q/train_gte50005.tmp\n",
      "PoolWorker-44: Wrote 1000 images to /tmp/madlib_CU2CNf1Yxz/train_gte50005.tmp\n",
      "PoolWorker-42: Loaded 404 images into train_gte5\n",
      "PoolWorker-43: Loaded 1000 images into train_gte5\n",
      "PoolWorker-44: Loaded 1000 images into train_gte5\n",
      "PoolWorker-45: Removed temporary directory PoolWorker-45\n",
      "PoolWorker-44: Removed temporary directory PoolWorker-44\n",
      "PoolWorker-42: Removed temporary directory PoolWorker-42\n",
      "PoolWorker-41: Removed temporary directory PoolWorker-41\n",
      "PoolWorker-43: Removed temporary directory PoolWorker-43\n",
      "Done!  Loaded 29404 images in 12.6191689968s\n",
      "MainProcess: Connected to madlib db.\n",
      "Executing: CREATE TABLE test_gte5 (id SERIAL, x REAL[], y TEXT)\n",
      "CREATE TABLE\n",
      "Created table test_gte5 in madlib db\n",
      "Spawning 5 workers...\n",
      "Initializing PoolWorker-46 [pid 36521]\n",
      "PoolWorker-46: Created temporary directory PoolWorker-46\n",
      "Initializing PoolWorker-47 [pid 36522]\n",
      "PoolWorker-47: Created temporary directory PoolWorker-47\n",
      "Initializing PoolWorker-48 [pid 36523]\n",
      "PoolWorker-50: Created temporary directory PoolWorker-50\n",
      "PoolWorker-48: Created temporary directory PoolWorker-48\n",
      "Initializing PoolWorker-49 [pid 36524]\n",
      "PoolWorker-49: Created temporary directory PoolWorker-49\n",
      "Initializing PoolWorker-50 [pid 36525]\n",
      "PoolWorker-46: Connected to madlib db.\n",
      "PoolWorker-47: Connected to madlib db.\n",
      "PoolWorker-48: Connected to madlib db.\n",
      "PoolWorker-49: Connected to madlib db.\n",
      "PoolWorker-50: Connected to madlib db.\n",
      "PoolWorker-46: Wrote 1000 images to /tmp/madlib_DTfIW4fes1/test_gte50000.tmp\n",
      "PoolWorker-50: Wrote 861 images to /tmp/madlib_dkZDQsdCfI/test_gte50000.tmp\n",
      "PoolWorker-48: Wrote 1000 images to /tmp/madlib_eLa5ZPTJoU/test_gte50000.tmp\n",
      "PoolWorker-47: Wrote 1000 images to /tmp/madlib_agBlKzPnjs/test_gte50000.tmp\n",
      "PoolWorker-49: Wrote 1000 images to /tmp/madlib_89sgwkcIn8/test_gte50000.tmp\n",
      "PoolWorker-50: Loaded 861 images into test_gte5\n",
      "PoolWorker-48: Loaded 1000 images into test_gte5\n",
      "PoolWorker-47: Loaded 1000 images into test_gte5\n",
      "PoolWorker-49: Loaded 1000 images into test_gte5\n",
      "PoolWorker-46: Loaded 1000 images into test_gte5\n",
      "PoolWorker-50: Removed temporary directory PoolWorker-50\n",
      "PoolWorker-49: Removed temporary directory PoolWorker-49\n",
      "PoolWorker-46: Removed temporary directory PoolWorker-46\n",
      "PoolWorker-47: Removed temporary directory PoolWorker-47\n",
      "PoolWorker-48: Removed temporary directory PoolWorker-48\n",
      "Done!  Loaded 4861 images in 3.05431604385s\n"
     ]
    }
   ],
   "source": [
    "# Drop tables\n",
    "%sql DROP TABLE IF EXISTS train_lt5, test_lt5, train_gte5, test_gte5\n",
    "\n",
    "# Save images to temporary directories and load into database\n",
    "iloader.load_np_array_to_table(x_train_lt5, y_train_lt5, 'train_lt5', append=False, img_names=None)\n",
    "iloader.load_np_array_to_table(x_test_lt5, y_test_lt5, 'test_lt5', append=False, img_names=None)\n",
    "iloader.load_np_array_to_table(x_train_gte5, y_train_gte5, 'train_gte5', append=False, img_names=None)\n",
    "iloader.load_np_array_to_table(x_test_gte5, y_test_gte5, 'test_gte5', append=False, img_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"image_preproc\"></a>\n",
    "# 3. Call image preprocessor\n",
    "\n",
    "Transforms from one image per row to multiple images per row for batch optimization.  Also normalizes and one-hot encodes.\n",
    "\n",
    "Training dataset < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "1 rows affected.\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>source_table</th>\n",
       "        <th>output_table</th>\n",
       "        <th>dependent_varname</th>\n",
       "        <th>independent_varname</th>\n",
       "        <th>dependent_vartype</th>\n",
       "        <th>class_values</th>\n",
       "        <th>buffer_size</th>\n",
       "        <th>normalizing_const</th>\n",
       "        <th>num_classes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>train_lt5</td>\n",
       "        <td>train_lt5_packed</td>\n",
       "        <td>y</td>\n",
       "        <td>x</td>\n",
       "        <td>text</td>\n",
       "        <td>[u'0', u'1', u'2', u'3', u'4']</td>\n",
       "        <td>15298</td>\n",
       "        <td>255.0</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(u'train_lt5', u'train_lt5_packed', u'y', u'x', u'text', [u'0', u'1', u'2', u'3', u'4'], 15298, 255.0, 5)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS train_lt5_packed, train_lt5_packed_summary;\n",
    "\n",
    "SELECT madlib.training_preprocessor_dl('train_lt5',               -- Source table\n",
    "                                       'train_lt5_packed',        -- Output table\n",
    "                                       'y',                       -- Dependent variable\n",
    "                                       'x',                       -- Independent variable\n",
    "                                        NULL,                     -- Buffer size\n",
    "                                        255                       -- Normalizing constant\n",
    "                                        );\n",
    "\n",
    "SELECT * FROM train_lt5_packed_summary;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test dataset < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "1 rows affected.\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>source_table</th>\n",
       "        <th>output_table</th>\n",
       "        <th>dependent_varname</th>\n",
       "        <th>independent_varname</th>\n",
       "        <th>dependent_vartype</th>\n",
       "        <th>class_values</th>\n",
       "        <th>buffer_size</th>\n",
       "        <th>normalizing_const</th>\n",
       "        <th>num_classes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>test_lt5</td>\n",
       "        <td>test_lt5_packed</td>\n",
       "        <td>y</td>\n",
       "        <td>x</td>\n",
       "        <td>text</td>\n",
       "        <td>[u'0', u'1', u'2', u'3', u'4']</td>\n",
       "        <td>2570</td>\n",
       "        <td>255.0</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(u'test_lt5', u'test_lt5_packed', u'y', u'x', u'text', [u'0', u'1', u'2', u'3', u'4'], 2570, 255.0, 5)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS test_lt5_packed, test_lt5_packed_summary;\n",
    "\n",
    "SELECT madlib.validation_preprocessor_dl('test_lt5',                -- Source table\n",
    "                                         'test_lt5_packed',         -- Output table\n",
    "                                         'y',                       -- Dependent variable\n",
    "                                         'x',                       -- Independent variable\n",
    "                                         'train_lt5_packed'         -- Training preproc table\n",
    "                                        );\n",
    "\n",
    "SELECT * FROM test_lt5_packed_summary;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training dataset >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "1 rows affected.\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>source_table</th>\n",
       "        <th>output_table</th>\n",
       "        <th>dependent_varname</th>\n",
       "        <th>independent_varname</th>\n",
       "        <th>dependent_vartype</th>\n",
       "        <th>class_values</th>\n",
       "        <th>buffer_size</th>\n",
       "        <th>normalizing_const</th>\n",
       "        <th>num_classes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>train_gte5</td>\n",
       "        <td>train_gte5_packed</td>\n",
       "        <td>y</td>\n",
       "        <td>x</td>\n",
       "        <td>text</td>\n",
       "        <td>[u'0', u'1', u'2', u'3', u'4']</td>\n",
       "        <td>14702</td>\n",
       "        <td>255.0</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(u'train_gte5', u'train_gte5_packed', u'y', u'x', u'text', [u'0', u'1', u'2', u'3', u'4'], 14702, 255.0, 5)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS train_gte5_packed, train_gte5_packed_summary;\n",
    "\n",
    "SELECT madlib.training_preprocessor_dl('train_gte5',              -- Source table\n",
    "                                       'train_gte5_packed',       -- Output table\n",
    "                                       'y',                       -- Dependent variable\n",
    "                                       'x',                       -- Independent variable\n",
    "                                        NULL,                     -- Buffer size\n",
    "                                        255                       -- Normalizing constant\n",
    "                                        );\n",
    "\n",
    "SELECT * FROM train_gte5_packed_summary;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test dataset >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "1 rows affected.\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>source_table</th>\n",
       "        <th>output_table</th>\n",
       "        <th>dependent_varname</th>\n",
       "        <th>independent_varname</th>\n",
       "        <th>dependent_vartype</th>\n",
       "        <th>class_values</th>\n",
       "        <th>buffer_size</th>\n",
       "        <th>normalizing_const</th>\n",
       "        <th>num_classes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>test_gte5</td>\n",
       "        <td>test_gte5_packed</td>\n",
       "        <td>y</td>\n",
       "        <td>x</td>\n",
       "        <td>text</td>\n",
       "        <td>[u'0', u'1', u'2', u'3', u'4']</td>\n",
       "        <td>2431</td>\n",
       "        <td>255.0</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(u'test_gte5', u'test_gte5_packed', u'y', u'x', u'text', [u'0', u'1', u'2', u'3', u'4'], 2431, 255.0, 5)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS test_gte5_packed, test_gte5_packed_summary;\n",
    "\n",
    "SELECT madlib.validation_preprocessor_dl('test_gte5',             -- Source table\n",
    "                                         'test_gte5_packed',      -- Output table\n",
    "                                         'y',                     -- Dependent variable\n",
    "                                         'x',                     -- Independent variable\n",
    "                                         'train_gte5_packed'      -- Training preproc table\n",
    "                                        );\n",
    "\n",
    "SELECT * FROM test_gte5_packed_summary;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define_and_load_model\"></a>\n",
    "# 4. Define and load model architecture\n",
    "\n",
    "Model with feature and classification layers trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 600,165\n",
      "Trainable params: 600,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define two groups of layers: feature (convolutions) and classification (dense)\n",
    "feature_layers = [\n",
    "    Conv2D(filters, kernel_size,\n",
    "           padding='valid',\n",
    "           input_shape=input_shape),\n",
    "    Activation('relu'),\n",
    "    Conv2D(filters, kernel_size),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "]\n",
    "\n",
    "classification_layers = [\n",
    "    Dense(128),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes),\n",
    "    Activation('softmax')\n",
    "]\n",
    "\n",
    "# create complete model\n",
    "model = Sequential(feature_layers + classification_layers)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load into model architecture table using psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>model_id</th>\n",
       "        <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1</td>\n",
       "        <td>feature + classification layers trainable</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, u'feature + classification layers trainable')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg2 as p2\n",
    "conn = p2.connect('postgresql://gpadmin@35.239.240.26:5432/madlib')\n",
    "cur = conn.cursor()\n",
    "\n",
    "%sql DROP TABLE IF EXISTS model_arch_library;\n",
    "query = \"SELECT madlib.load_keras_model('model_arch_library', %s, NULL, %s)\"\n",
    "cur.execute(query,[model.to_json(), \"feature + classification layers trainable\"])\n",
    "conn.commit()\n",
    "\n",
    "# check model loaded OK\n",
    "%sql SELECT model_id, name FROM model_arch_library;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with feature layers frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 600,165\n",
      "Trainable params: 590,597\n",
      "Non-trainable params: 9,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# freeze feature layers\n",
    "for l in feature_layers:\n",
    "    l.trainable = False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load into transfer model architecture table using psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>model_id</th>\n",
       "        <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>1</td>\n",
       "        <td>feature + classification layers trainable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2</td>\n",
       "        <td>only classification layers trainable</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(1, u'feature + classification layers trainable'),\n",
       " (2, u'only classification layers trainable')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(query,[model.to_json(), \"only classification layers trainable\"])\n",
    "conn.commit()\n",
    "\n",
    "# check model loaded OK\n",
    "%sql SELECT model_id, name FROM model_arch_library ORDER BY model_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "# 5.  Train\n",
    "Train the model for 5-digit classification [0..4]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>madlib_keras_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('',)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS mnist_model, mnist_model_summary;\n",
    "\n",
    "SELECT madlib.madlib_keras_fit('train_lt5_packed',    -- source table\n",
    "                               'mnist_model',         -- model output table\n",
    "                               'model_arch_library',  -- model arch table\n",
    "                                1,                    -- model arch id\n",
    "                                $$ loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']$$,  -- compile_params\n",
    "                                $$ batch_size=128, epochs=1 $$,  -- fit_params\n",
    "                                5                     -- num_iterations\n",
    "                              );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>source_table</th>\n",
       "        <th>model</th>\n",
       "        <th>dependent_varname</th>\n",
       "        <th>independent_varname</th>\n",
       "        <th>model_arch_table</th>\n",
       "        <th>model_arch_id</th>\n",
       "        <th>compile_params</th>\n",
       "        <th>fit_params</th>\n",
       "        <th>num_iterations</th>\n",
       "        <th>validation_table</th>\n",
       "        <th>metrics_compute_frequency</th>\n",
       "        <th>name</th>\n",
       "        <th>description</th>\n",
       "        <th>model_type</th>\n",
       "        <th>model_size</th>\n",
       "        <th>start_training_time</th>\n",
       "        <th>end_training_time</th>\n",
       "        <th>metrics_elapsed_time</th>\n",
       "        <th>madlib_version</th>\n",
       "        <th>num_classes</th>\n",
       "        <th>class_values</th>\n",
       "        <th>dependent_vartype</th>\n",
       "        <th>normalizing_const</th>\n",
       "        <th>metrics_type</th>\n",
       "        <th>training_metrics_final</th>\n",
       "        <th>training_loss_final</th>\n",
       "        <th>training_metrics</th>\n",
       "        <th>training_loss</th>\n",
       "        <th>validation_metrics_final</th>\n",
       "        <th>validation_loss_final</th>\n",
       "        <th>validation_metrics</th>\n",
       "        <th>validation_loss</th>\n",
       "        <th>metrics_iters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>train_lt5_packed</td>\n",
       "        <td>mnist_model</td>\n",
       "        <td>y</td>\n",
       "        <td>x</td>\n",
       "        <td>model_arch_library</td>\n",
       "        <td>1</td>\n",
       "        <td> loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']</td>\n",
       "        <td> batch_size=128, epochs=1 </td>\n",
       "        <td>5</td>\n",
       "        <td>None</td>\n",
       "        <td>5</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "        <td>madlib_keras</td>\n",
       "        <td>2344.43066406</td>\n",
       "        <td>2019-06-24 19:08:31.328530</td>\n",
       "        <td>2019-06-24 19:13:50.944601</td>\n",
       "        <td>[319.616029977798]</td>\n",
       "        <td>1.16-dev</td>\n",
       "        <td>5</td>\n",
       "        <td>[u'0', u'1', u'2', u'3', u'4']</td>\n",
       "        <td>text</td>\n",
       "        <td>255.0</td>\n",
       "        <td>[u'accuracy']</td>\n",
       "        <td>0.996045231819</td>\n",
       "        <td>0.0139331035316</td>\n",
       "        <td>[0.996045231819153]</td>\n",
       "        <td>[0.013933103531599]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "        <td>[5]</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(u'train_lt5_packed', u'mnist_model', u'y', u'x', u'model_arch_library', 1, u\" loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']\", u' batch_size=128, epochs=1 ', 5, None, 5, None, None, u'madlib_keras', 2344.43066406, datetime.datetime(2019, 6, 24, 19, 8, 31, 328530), datetime.datetime(2019, 6, 24, 19, 13, 50, 944601), [319.616029977798], u'1.16-dev', 5, [u'0', u'1', u'2', u'3', u'4'], u'text', 255.0, [u'accuracy'], 0.996045231819, 0.0139331035316, [0.996045231819153], [0.013933103531599], None, None, None, None, [5])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * FROM mnist_model_summary;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "1 rows affected.\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>loss</th>\n",
       "        <th>metric</th>\n",
       "        <th>metrics_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0.00919340737164</td>\n",
       "        <td>0.997081160545</td>\n",
       "        <td>[u'accuracy']</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(0.00919340737164021, 0.997081160545349, [u'accuracy'])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS mnist_validate;\n",
    "\n",
    "SELECT madlib.madlib_keras_evaluate('mnist_model',      -- model\n",
    "                                   'test_lt5_packed',   -- test table\n",
    "                                   'mnist_validate'     -- output table\n",
    "                                   );\n",
    "\n",
    "SELECT * FROM mnist_validate;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transfer_learning\"></a>\n",
    "# 6. Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use UPDATE to load trained weights from previous run into the model library table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "UPDATE model_arch_library SET model_weights = model_data FROM mnist_model WHERE model_id = 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer: train dense layers for new classification task [5..9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>madlib_keras_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td></td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('',)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS mnist_transfer_model, mnist_transfer_model_summary;\n",
    "\n",
    "SELECT madlib.madlib_keras_fit('train_gte5_packed',   -- source table\n",
    "                               'mnist_transfer_model',-- model output table\n",
    "                               'model_arch_library',  -- model arch table\n",
    "                                2,                    -- model arch id\n",
    "                                $$ loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']$$,  -- compile_params\n",
    "                                $$ batch_size=128, epochs=1 $$,  -- fit_params\n",
    "                                5                     -- num_iterations\n",
    "                              );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>source_table</th>\n",
       "        <th>model</th>\n",
       "        <th>dependent_varname</th>\n",
       "        <th>independent_varname</th>\n",
       "        <th>model_arch_table</th>\n",
       "        <th>model_arch_id</th>\n",
       "        <th>compile_params</th>\n",
       "        <th>fit_params</th>\n",
       "        <th>num_iterations</th>\n",
       "        <th>validation_table</th>\n",
       "        <th>metrics_compute_frequency</th>\n",
       "        <th>name</th>\n",
       "        <th>description</th>\n",
       "        <th>model_type</th>\n",
       "        <th>model_size</th>\n",
       "        <th>start_training_time</th>\n",
       "        <th>end_training_time</th>\n",
       "        <th>metrics_elapsed_time</th>\n",
       "        <th>madlib_version</th>\n",
       "        <th>num_classes</th>\n",
       "        <th>class_values</th>\n",
       "        <th>dependent_vartype</th>\n",
       "        <th>normalizing_const</th>\n",
       "        <th>metrics_type</th>\n",
       "        <th>training_metrics_final</th>\n",
       "        <th>training_loss_final</th>\n",
       "        <th>training_metrics</th>\n",
       "        <th>training_loss</th>\n",
       "        <th>validation_metrics_final</th>\n",
       "        <th>validation_loss_final</th>\n",
       "        <th>validation_metrics</th>\n",
       "        <th>validation_loss</th>\n",
       "        <th>metrics_iters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>train_gte5_packed</td>\n",
       "        <td>mnist_transfer_model</td>\n",
       "        <td>y</td>\n",
       "        <td>x</td>\n",
       "        <td>model_arch_library</td>\n",
       "        <td>2</td>\n",
       "        <td> loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']</td>\n",
       "        <td> batch_size=128, epochs=1 </td>\n",
       "        <td>5</td>\n",
       "        <td>None</td>\n",
       "        <td>5</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "        <td>madlib_keras</td>\n",
       "        <td>2344.43066406</td>\n",
       "        <td>2019-06-24 19:16:55.336042</td>\n",
       "        <td>2019-06-24 19:19:53.589704</td>\n",
       "        <td>[178.253571987152]</td>\n",
       "        <td>1.16-dev</td>\n",
       "        <td>5</td>\n",
       "        <td>[u'0', u'1', u'2', u'3', u'4']</td>\n",
       "        <td>text</td>\n",
       "        <td>255.0</td>\n",
       "        <td>[u'accuracy']</td>\n",
       "        <td>0.991429746151</td>\n",
       "        <td>0.0280887652189</td>\n",
       "        <td>[0.99142974615097]</td>\n",
       "        <td>[0.028088765218854]</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "        <td>[5]</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(u'train_gte5_packed', u'mnist_transfer_model', u'y', u'x', u'model_arch_library', 2, u\" loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy']\", u' batch_size=128, epochs=1 ', 5, None, 5, None, None, u'madlib_keras', 2344.43066406, datetime.datetime(2019, 6, 24, 19, 16, 55, 336042), datetime.datetime(2019, 6, 24, 19, 19, 53, 589704), [178.253571987152], u'1.16-dev', 5, [u'0', u'1', u'2', u'3', u'4'], u'text', 255.0, [u'accuracy'], 0.991429746151, 0.0280887652189, [0.99142974615097], [0.028088765218854], None, None, None, None, [5])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT * FROM mnist_transfer_model_summary;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "1 rows affected.\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>loss</th>\n",
       "        <th>metric</th>\n",
       "        <th>metrics_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>0.0312170274556</td>\n",
       "        <td>0.989714026451</td>\n",
       "        <td>[u'accuracy']</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(0.0312170274555683, 0.989714026451111, [u'accuracy'])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS mnist_transfer_validate;\n",
    "\n",
    "SELECT madlib.madlib_keras_evaluate('mnist_transfer_model',      -- model\n",
    "                                   'test_gte5_packed',           -- test table\n",
    "                                   'mnist_transfer_validate'     -- output table\n",
    "                                   );\n",
    "\n",
    "SELECT * FROM mnist_transfer_validate;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
